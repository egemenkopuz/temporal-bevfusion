{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.datasets import build_dataset, build_dataloader\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.utils import recursive_eval\n",
    "from mmcv import Config\n",
    "from torchpack.utils.config import configs\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmdet.datasets import DATASETS\n",
    "from mmdet3d.datasets.dataset_wrappers import CBGSDataset\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from mmdet3d.core.utils import visualize_lidar\n",
    "from mmdet3d.core.utils.visualize import visualize_lidar_combined, visualize_prev_lidar_combined\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_pipeline(pipeline: list, type: str) -> int:\n",
    "    for i, p in enumerate(pipeline):\n",
    "        if p[\"type\"] == type:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def run_temporal_loading(cfg, dataloader, stop_batch_idx=None, save_path=None, custom_cloud_point_ranges=None):\n",
    "    if cfg.seed is not None:\n",
    "        random.seed(cfg.seed)\n",
    "        np.random.seed(cfg.seed)\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "    gtp_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ObjectPaste\")\n",
    "    sampled = cfg.data.train.dataset.pipeline[gtp_idx].stop_epoch > 0\n",
    "\n",
    "    if custom_cloud_point_ranges is not None:\n",
    "        cfg.point_cloud_range = custom_cloud_point_ranges\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "        frame_idxs = [x[\"frame_idx\"] for x in batch_data[\"metas\"].data[0][0]]\n",
    "        print(f\"batch_idx: {batch_idx:<4} seq_indices:\", frame_idxs)\n",
    "\n",
    "        if save_path is not None:\n",
    "            save_visuals(cfg, batch_idx, batch_data, save_path, f)\n",
    "\n",
    "        if stop_batch_idx is not None and batch_idx == stop_batch_idx:\n",
    "            return\n",
    "\n",
    "def run_index_loading(cfg, dataset, index=0, save_path=None, custom_cloud_point_ranges=None):\n",
    "    if cfg.seed is not None:\n",
    "        random.seed(cfg.seed)\n",
    "        np.random.seed(cfg.seed)\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "    gtp_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ObjectPaste\")\n",
    "    sampled = cfg.data.train.dataset.pipeline[gtp_idx].stop_epoch > 0\n",
    "\n",
    "    input_dict = dataset.dataset.get_data_info(index)\n",
    "    dataset.dataset.pre_pipeline(input_dict)\n",
    "    input_dict = dataset.dataset.pipeline(input_dict)\n",
    "    frame_idxs = input_dict[\"metas\"].data[\"frame_idx\"]\n",
    "    print(f\"batch_idx: {index:<4} seq_indices:\", frame_idxs)\n",
    "\n",
    "    if custom_cloud_point_ranges is not None:\n",
    "        cfg.point_cloud_range = custom_cloud_point_ranges\n",
    "\n",
    "    lidar_root_combined_path = os.path.join(save_path, \"lidar-combined\")\n",
    "    lidar_root_classes_path = os.path.join(save_path, \"lidar-classes\")\n",
    "    save_id = f\"{str(index).zfill(4)}.jpg\"\n",
    "\n",
    "    img = input_dict[\"img\"].data\n",
    "    points = input_dict[\"points\"].data\n",
    "    metas = input_dict[\"metas\"].data\n",
    "    gt_bboxes_3d = input_dict[\"gt_bboxes_3d\"].data\n",
    "    gt_labels_3d = input_dict[\"gt_labels_3d\"].data\n",
    "\n",
    "    len_queue = img.size(0)\n",
    "    len_img = img.size(1)\n",
    "    sampled_bboxes_3d = copy.deepcopy(gt_bboxes_3d)\n",
    "\n",
    "    all_bboxes_3d = copy.deepcopy(gt_bboxes_3d)\n",
    "    all_labels_3d = copy.deepcopy(gt_labels_3d)\n",
    "    for i in range(len_queue):\n",
    "        if sampled:\n",
    "            sample_mode:list = metas[i][\"sample_mode\"] # list of 0 and 1\n",
    "            # switch 0 and 1s\n",
    "            mask = np.array(~sample_mode + 2, dtype=np.bool_)\n",
    "            gt_bboxes_3d[i].tensor = copy.deepcopy(all_bboxes_3d[i].tensor[mask])\n",
    "            sampled_bboxes_3d[i].tensor = copy.deepcopy(all_bboxes_3d[i].tensor[~mask])\n",
    "            gt_labels_3d[i] = all_labels_3d[i][mask]\n",
    "\n",
    "    visualize_lidar(\n",
    "        os.path.join(lidar_root_classes_path, save_id),\n",
    "        points,\n",
    "        bboxes=all_bboxes_3d,\n",
    "        labels=all_labels_3d,\n",
    "        classes=cfg.object_classes,\n",
    "        dataset=cfg.data.train.dataset.type,\n",
    "        xlim=[cfg.point_cloud_range[d] for d in [0, 3]],\n",
    "        ylim=[cfg.point_cloud_range[d] for d in [1, 4]],\n",
    "        thickness=10,\n",
    "        add_coordinate_lines=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def save_visuals(cfg, batch_idx:int, batch_data: dict, target_path: str, sampled:bool) -> None:\n",
    "    save_dpi = 20\n",
    "    batch_size = batch_data[\"img\"].data[0].size(0)\n",
    "\n",
    "    for batch_i_idx in range(batch_size):\n",
    "        img = batch_data[\"img\"].data[0][batch_i_idx]\n",
    "        points = batch_data[\"points\"].data[0][batch_i_idx]\n",
    "        metas = batch_data[\"metas\"].data[0][batch_i_idx]\n",
    "        gt_bboxes_3d = batch_data[\"gt_bboxes_3d\"].data[0][batch_i_idx]\n",
    "        gt_labels_3d = batch_data[\"gt_labels_3d\"].data[0][batch_i_idx]\n",
    "\n",
    "        # Temporal Frame, Image, C, H, W\n",
    "        assert len(img.shape) == 5, img.shape\n",
    "\n",
    "        len_queue = img.size(0)\n",
    "        len_img = img.size(1)\n",
    "        sampled_bboxes_3d = copy.deepcopy(gt_bboxes_3d)\n",
    "        save_root_id = f\"b{str(batch_idx).zfill(4)}_{str(batch_i_idx).zfill(2)}\"\n",
    "        seq_indices = [str(x[\"frame_idx\"]).zfill(3) for x in metas]\n",
    "\n",
    "        lidar_root_combined_path = os.path.join(target_path, \"lidar-combined\")\n",
    "        lidar_root_classes_path = os.path.join(target_path, \"lidar-classes\")\n",
    "        lidar_root_with_prev = os.path.join(target_path, \"lidar-with-prev\")\n",
    "        img_root_paths = [os.path.join(target_path, f\"images-{img_id}\") for img_id in range(len_img)]\n",
    "        [os.makedirs(x, exist_ok=True) for x in img_root_paths]\n",
    "\n",
    "        all_bboxes_3d = copy.deepcopy(gt_bboxes_3d)\n",
    "        all_labels_3d = copy.deepcopy(gt_labels_3d)\n",
    "        for i in range(len_queue):\n",
    "            if sampled:\n",
    "                sample_mode:list = metas[i][\"sample_mode\"] # list of 0 and 1\n",
    "                # switch 0 and 1s\n",
    "                mask = np.array(~sample_mode + 2, dtype=np.bool_)\n",
    "                gt_bboxes_3d[i].tensor = copy.deepcopy(all_bboxes_3d[i].tensor[mask])\n",
    "                sampled_bboxes_3d[i].tensor = copy.deepcopy(all_bboxes_3d[i].tensor[~mask])\n",
    "                gt_labels_3d[i] = all_labels_3d[i][mask]\n",
    "\n",
    "\n",
    "        for i in range(len_queue):\n",
    "            save_id = f\"{save_root_id}_{seq_indices[i]}.jpg\"\n",
    "            for img_id in range(img.shape[1]):\n",
    "                img_ = img[i][img_id]\n",
    "                save_image(img_, os.path.join(img_root_paths[img_id], save_id))\n",
    "\n",
    "            visualize_lidar(\n",
    "                os.path.join(lidar_root_classes_path, save_id),\n",
    "                points[i],\n",
    "                bboxes=all_bboxes_3d[i],\n",
    "                labels=all_labels_3d[i],\n",
    "                classes=cfg.object_classes,\n",
    "                dataset=cfg.data.train.dataset.type,\n",
    "                xlim=[cfg.point_cloud_range[d] for d in [0, 3]],\n",
    "                ylim=[cfg.point_cloud_range[d] for d in [1, 4]],\n",
    "                thickness=10,\n",
    "                save_dpi=save_dpi,\n",
    "            )\n",
    "\n",
    "            if sampled:\n",
    "                visualize_lidar_combined(\n",
    "                    os.path.join(lidar_root_combined_path, save_id),\n",
    "                    points[i],\n",
    "                    pred_bboxes=sampled_bboxes_3d[i],\n",
    "                    gt_bboxes=gt_bboxes_3d[i],\n",
    "                    xlim=[cfg.point_cloud_range[d] for d in [0, 3]],\n",
    "                    ylim=[cfg.point_cloud_range[d] for d in [1, 4]],\n",
    "                    thickness=10,\n",
    "                    save_dpi=save_dpi,\n",
    "                )\n",
    "\n",
    "\n",
    "        if sampled:\n",
    "            colors = [\n",
    "                (8, 69, 148),\n",
    "                (66, 146, 198),\n",
    "                (158, 202, 225),\n",
    "            ]\n",
    "            visualize_prev_lidar_combined(\n",
    "                os.path.join(lidar_root_with_prev, f\"{save_root_id}_{seq_indices[-1]}.jpg\"),\n",
    "                points[-1],\n",
    "                bboxes=sampled_bboxes_3d,\n",
    "                xlim=[cfg.point_cloud_range[d] for d in [0, 3]],\n",
    "                ylim=[cfg.point_cloud_range[d] for d in [1, 4]],\n",
    "                thickness=10,\n",
    "                colors=colors,\n",
    "                save_dpi=save_dpi,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0    seq_indices: [61, 62, 63]\n",
      "batch_idx: 1    seq_indices: [163, 165, 166]\n"
     ]
    }
   ],
   "source": [
    "config_path = \"configs/tumtraf-i/temporal/transfusion/lidar/voxelnet-convlstm-1600g-0xy1-0z20-sameaugall-ql3-qrt1-gtp3-sameaug-trans-rot-lfrz.yaml\"\n",
    "configs.load(config_path, recursive=True)\n",
    "cfg = Config(recursive_eval(configs), filename=config_path)\n",
    "\n",
    "copy_cfg = copy.deepcopy(cfg)\n",
    "\n",
    "# disable ImageNormalize\n",
    "in_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ImageNormalize\")\n",
    "copy_cfg.data.train.dataset.pipeline[in_idx][\"skip_normalize\"] = True\n",
    "\n",
    "# find ImageAug3D\n",
    "ia_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ImageAug3D\")\n",
    "copy_cfg.data.train.dataset.pipeline[ia_idx][\"is_train\"] = False\n",
    "\n",
    "# find GridMask\n",
    "gm_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"GridMask\")\n",
    "copy_cfg.data.train.dataset.pipeline[gm_idx][\"prob\"] = 0.0\n",
    "copy_cfg.data.train.dataset.pipeline[gm_idx][\"max_epoch\"] = 99\n",
    "\n",
    "# # find RandomFlip3D\n",
    "rf_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"RandomFlip3D\")\n",
    "copy_cfg.data.train.dataset.pipeline[rf_idx][\"flip_horizontal\"] = False\n",
    "copy_cfg.data.train.dataset.pipeline[rf_idx][\"flip_vertical\"] = False\n",
    "\n",
    "# # find GlobalRotScaleTrans\n",
    "grst_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"GlobalRotScaleTrans\")\n",
    "copy_cfg.data.train.dataset.pipeline[grst_idx][\"is_train\"] = True\n",
    "copy_cfg.data.train.dataset.pipeline[grst_idx][\"resize_lim\"] = [1.0, 1.0]\n",
    "copy_cfg.data.train.dataset.pipeline[grst_idx][\"rot_lim\"] = [0.0, 0.0]\n",
    "copy_cfg.data.train.dataset.pipeline[grst_idx][\"trans_lim\"] = 0.0\n",
    "\n",
    "gtp_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ObjectPaste\")\n",
    "copy_cfg.data.train.dataset.pipeline[gtp_idx].stop_epoch = 99\n",
    "copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.cls_rot_lim = {\n",
    "    \"CAR\": [\"normal\", 0.0, 0.05],\n",
    "    \"TRAILER\": [\"normal\", 0.0, 0.05],\n",
    "    \"TRUCK\": [\"normal\", 0.0, 0.05],\n",
    "    \"VAN\": [\"normal\", 0.0, 0.05],\n",
    "    \"PEDESTRIAN\": [\"normal\", 0.0, 0.0],\n",
    "    \"BUS\": [\"normal\", 0.0, 0.05],\n",
    "    \"MOTORCYCLE\": [\"normal\", 0.0, 0.0],\n",
    "    \"BICYCLE\": [\"normal\", 0.0, 0.05],\n",
    "    \"EMERGENCY_VEHICLE\": [\"normal\", 0.0, 0.05],\n",
    "}\n",
    "copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.cls_trans_lim = {\n",
    "    \"CAR\": [\"uniform\", 0.0, 2.0],\n",
    "    \"TRAILER\": [\"uniform\", 0.4, 2.0],\n",
    "    \"TRUCK\": [\"uniform\", 0.4, 0.6],\n",
    "    \"VAN\": [\"uniform\", 0.4, 0.4],\n",
    "    \"PEDESTRIAN\": [\"uniform\", 0.0, 0.0],\n",
    "    \"BUS\": [\"uniform\", 0.4, 1.5],\n",
    "    \"MOTORCYCLE\": [\"uniform\", 0.0, 0.0],\n",
    "    \"BICYCLE\": [\"uniform\", 0.4, 0.8],\n",
    "    \"EMERGENCY_VEHICLE\": [\"uniform\", 0.4, 0.8],\n",
    "}\n",
    "\n",
    "# copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.reduce_points_by_distance.prob = 0.5\n",
    "# copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.reduce_points_by_distance.distance_threshold = 200\n",
    "# copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.reduce_points_by_distance.max_ratio = 0.6\n",
    "\n",
    "dataset = build_dataset(copy_cfg.data.train, dict(test_mode=False))\n",
    "dataloader = build_dataloader(dataset, 1, 1, 1, dist=True, seed=copy_cfg.seed, shuffle=True)\n",
    "\n",
    "save_path = os.path.join(\"debugging-results\", \"tumtraf-i-temporal-loading\")\n",
    "custom_cloud_point_ranges = [-5.0, -45.0, -10.0, 85.0, 45.0, 0.0]\n",
    "\n",
    "os.system(f\"rm -rf {save_path}\")\n",
    "# run_index_loading(copy_cfg, dataset, index=0, save_path=save_path, custom_cloud_point_ranges=custom_cloud_point_ranges)\n",
    "run_temporal_loading(copy_cfg, dataloader, stop_batch_idx=1, save_path=save_path, custom_cloud_point_ranges=custom_cloud_point_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "# dataloader = build_dataloader(dataset, 1, 4, 1, dist=True, seed=cfg.seed, shuffle=False)\n",
    "# run_temporal_loading(cfg, dataloader, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
