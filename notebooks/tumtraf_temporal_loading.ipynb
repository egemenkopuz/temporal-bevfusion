{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/mmdet3d\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.datasets import build_dataset, build_dataloader\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet3d.utils import recursive_eval\n",
    "from mmcv import Config\n",
    "from torchpack.utils.config import configs\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from mmdet.datasets import DATASETS\n",
    "from mmdet3d.datasets.dataset_wrappers import CBGSDataset\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "from mmdet3d.core.utils import visualize_lidar\n",
    "from mmdet3d.core.utils.visualize import visualize_lidar_combined\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_pipeline(pipeline: list, type: str) -> int:\n",
    "    for i, p in enumerate(pipeline):\n",
    "        if p[\"type\"] == type:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def run_temporal_loading(cfg, dataloader, stop_batch_idx=None, save_path=None):\n",
    "    if cfg.seed is not None:\n",
    "        random.seed(cfg.seed)\n",
    "        np.random.seed(cfg.seed)\n",
    "        torch.manual_seed(cfg.seed)\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "        frame_idxs = [x[\"frame_idx\"] for x in batch_data[\"metas\"].data[0][0]]\n",
    "        print(f\"batch_idx: {batch_idx:<4} seq_indices:\", frame_idxs)\n",
    "\n",
    "        if save_path is not None:\n",
    "            save_visuals(cfg, batch_idx, batch_data, save_path)\n",
    "\n",
    "        if stop_batch_idx is not None and batch_idx == stop_batch_idx:\n",
    "            return\n",
    "\n",
    "\n",
    "def save_visuals(cfg, batch_idx:int, batch_data: dict, target_path: str) -> None:\n",
    "    batch_size = batch_data[\"img\"].data[0].size(0)\n",
    "    for batch_i_idx in range(batch_size):\n",
    "        img = batch_data[\"img\"].data[0][batch_i_idx]\n",
    "        points = batch_data[\"points\"].data[0][batch_i_idx]\n",
    "        metas = batch_data[\"metas\"].data[0][batch_i_idx]\n",
    "        gt_bboxes_3d = batch_data[\"gt_bboxes_3d\"].data[0][batch_i_idx]\n",
    "        gt_labels_3d = batch_data[\"gt_labels_3d\"].data[0][batch_i_idx]\n",
    "\n",
    "        # Temporal Frame, Image, C, H, W\n",
    "        assert len(img.shape) == 5, img.shape\n",
    "\n",
    "        len_queue = img.size(0)\n",
    "        len_img = img.size(1)\n",
    "        sampled_bboxes_3d = copy.deepcopy(gt_bboxes_3d)\n",
    "        save_root_id = f\"b{str(batch_idx).zfill(4)}_{str(batch_i_idx).zfill(2)}\"\n",
    "        seq_indices = [str(x[\"frame_idx\"]).zfill(3) for x in metas]\n",
    "\n",
    "        lidar_root_combined_path = os.path.join(target_path, \"lidar-combined\")\n",
    "        lidar_root_classes_path = os.path.join(target_path, \"lidar-classes\")\n",
    "        img_root_paths = [os.path.join(target_path, f\"images-{img_id}\") for img_id in range(len_img)]\n",
    "        [os.makedirs(x, exist_ok=True) for x in img_root_paths]\n",
    "\n",
    "        all_bboxes_3d = copy.deepcopy(gt_bboxes_3d)\n",
    "        all_labels_3d = copy.deepcopy(gt_labels_3d)\n",
    "        for i in range(len_queue):\n",
    "            sample_mode:list = metas[i][\"sample_mode\"] # list of 0 and 1\n",
    "            # switch 0 and 1s\n",
    "            mask = np.array(~sample_mode + 2, dtype=np.bool_)\n",
    "            gt_bboxes_3d[i].tensor = copy.deepcopy(all_bboxes_3d[i].tensor[mask])\n",
    "            sampled_bboxes_3d[i].tensor = copy.deepcopy(all_bboxes_3d[i].tensor[~mask])\n",
    "            gt_labels_3d[i] = all_labels_3d[i][mask]\n",
    "\n",
    "\n",
    "        for i in range(len_queue):\n",
    "            save_id = f\"{save_root_id}_{seq_indices[i]}.jpg\"\n",
    "            for img_id in range(img.shape[1]):\n",
    "                img_ = img[i][img_id]\n",
    "                save_image(img_, os.path.join(img_root_paths[img_id], save_id))\n",
    "\n",
    "            # add only colorful of sampled as well\n",
    "\n",
    "            # visualize_lidar(\n",
    "            #     os.path.join(lidar_root_classes_path, save_id),\n",
    "            #     points[i],\n",
    "            #     bboxes=all_bboxes_3d[i],\n",
    "            #     labels=all_labels_3d[i],\n",
    "            #     classes=cfg.object_classes,\n",
    "            #     dataset=cfg.data.train.dataset.type,\n",
    "            #     xlim=[cfg.point_cloud_range[d] for d in [0, 3]],\n",
    "            #     ylim=[cfg.point_cloud_range[d] for d in [1, 4]],\n",
    "            # )\n",
    "\n",
    "            # visualize_lidar_combined(\n",
    "            #     os.path.join(lidar_root_combined_path, save_id),\n",
    "            #     points[i],\n",
    "            #     pred_bboxes=sampled_bboxes_3d[i],\n",
    "            #     gt_bboxes=gt_bboxes_3d[i],\n",
    "            #     xlim=[cfg.point_cloud_range[d] for d in [0, 3]],\n",
    "            #     ylim=[cfg.point_cloud_range[d] for d in [1, 4]],\n",
    "            # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = (\n",
    "    \"configs/tumtraf-i/temporal/transfusion/lidar/voxelnet-1600g-0xy1-0z20-ql3-qrt2-gtp15.yaml\"\n",
    ")\n",
    "configs.load(config_path, recursive=True)\n",
    "cfg = Config(recursive_eval(configs), filename=config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx: 0    seq_indices: [40, 41, 42]\n",
      "batch_idx: 1    seq_indices: [37, 39, 40]\n",
      "batch_idx: 2    seq_indices: [8, 9, 10]\n",
      "batch_idx: 3    seq_indices: [91, 92, 93]\n",
      "batch_idx: 4    seq_indices: [7, 8, 9]\n",
      "batch_idx: 5    seq_indices: [30, 32, 33]\n",
      "batch_idx: 6    seq_indices: [52, 53, 54]\n",
      "batch_idx: 7    seq_indices: [60, 61, 63]\n",
      "batch_idx: 8    seq_indices: [49, 50, 51]\n",
      "batch_idx: 9    seq_indices: [32, 33, 35]\n",
      "batch_idx: 10   seq_indices: [19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "copy_cfg = copy.deepcopy(cfg)\n",
    "\n",
    "# disable ImageNormalize\n",
    "# in_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ImageNormalize\")\n",
    "# copy_cfg.data.train.dataset.pipeline[in_idx][\"skip_normalize\"] = True\n",
    "\n",
    "# find GridMask\n",
    "gm_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"GridMask\")\n",
    "copy_cfg.data.train.dataset.pipeline[gm_idx][\"prob\"] = 1.0\n",
    "copy_cfg.data.train.dataset.pipeline[gm_idx][\"max_epoch\"] = 99\n",
    "\n",
    "# # find RandomFlip3D\n",
    "# rf_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"RandomFlip3D\")\n",
    "# copy_cfg.data.train.dataset.pipeline[rf_idx][\"flip_horizontal\"] = False\n",
    "\n",
    "# # find GlobalRotScaleTrans\n",
    "# grst_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"GlobalRotScaleTrans\")\n",
    "# copy_cfg.data.train.dataset.pipeline[grst_idx][\"is_train\"] = False\n",
    "\n",
    "# gtp_idx = find_in_pipeline(cfg.data.train.dataset.pipeline, \"ObjectPaste\")\n",
    "# copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.reduce_points_by_distance.prob = 0.5\n",
    "# copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.reduce_points_by_distance.distance_threshold = 200\n",
    "# copy_cfg.data.train.dataset.pipeline[gtp_idx].db_sampler.reduce_points_by_distance.max_ratio = 0.6\n",
    "\n",
    "dataset = build_dataset(copy_cfg.data.train, dict(test_mode=False))\n",
    "dataloader = build_dataloader(dataset, 1, 1, 1, dist=True, seed=copy_cfg.seed, shuffle=True)\n",
    "\n",
    "save_path = os.path.join(\"debugging-results\", \"tumtraf-i-temporal-loading\")\n",
    "os.system(f\"rm -rf {save_path}\")\n",
    "run_temporal_loading(cfg, dataloader, stop_batch_idx=10, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "# dataloader = build_dataloader(dataset, 1, 4, 1, dist=True, seed=cfg.seed, shuffle=False)\n",
    "# run_temporal_loading(cfg, dataloader, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
