{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpts_root_dir = \"checkpoints\"\n",
    "results_root_dir = \"results\"\n",
    "test_results_summary_dir = \"category-summaries\"\n",
    "visuals_folder_name = \"images\"\n",
    "\n",
    "ckpt_category = \"tumtraf-i\"\n",
    "\n",
    "test_results_filename = \"test_results.json\"\n",
    "test_results_csv_filename = \"test_results.csv\"\n",
    "benchmark_filename = \"benchmark.json\"\n",
    "override_test_results = False\n",
    "override_visuals = False\n",
    "override_videos = False\n",
    "override_benchmark = False\n",
    "visuals_camera_bbox_score: float = 0.15\n",
    "visuals_max_samples: int = None\n",
    "visuals_save_bboxes = True\n",
    "visuals_include_combined = True  # gt + pred\n",
    "\n",
    "ckpt_dir = os.path.join(ckpts_root_dir, ckpt_category)\n",
    "chkpts = glob(os.path.join(ckpt_dir, \"*\"))\n",
    "results_w_category_dir = os.path.join(results_root_dir, ckpt_category)\n",
    "\n",
    "test_results_compiled_filename = f\"{ckpt_category}.csv\"\n",
    "test_results_compiled_path = os.path.join(\n",
    "    results_root_dir,\n",
    "    test_results_summary_dir,\n",
    "    test_results_compiled_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in chkpts:\n",
    "    summary_path = os.path.join(\n",
    "        results_w_category_dir,\n",
    "        os.path.basename(x),\n",
    "        test_results_filename,\n",
    "    )\n",
    "\n",
    "    if not override_test_results and os.path.exists(summary_path):\n",
    "        print(\"Skipping, test results exist for\", x)\n",
    "        continue\n",
    "\n",
    "    cfg_path = os.path.join(x, \"configs.yaml\")\n",
    "    pth_path = os.path.join(x, \"latest.pth\")\n",
    "\n",
    "    command = f\"torchpack dist-run -np 1 python tools/test.py \\\n",
    "        {cfg_path} {pth_path} \\\n",
    "        --eval bbox --eval-options extensive_report=True \\\n",
    "        save_summary_path={summary_path} \\\n",
    "        > /dev/null 2>&1\"\n",
    "    \n",
    "    print(\"Running tests for\", x)\n",
    "    os.system(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create images\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in chkpts:\n",
    "    cfg_path = os.path.join(x, \"configs.yaml\")\n",
    "    pth_path = os.path.join(x, \"latest.pth\")\n",
    "    id = x.split(\"/\")[-1]\n",
    "    \n",
    "    out_dir = os.path.join(results_w_category_dir, id, visuals_folder_name)\n",
    "    bboxes_dir = os.path.join(results_w_category_dir, id, \"bboxes-pred\")\n",
    "    labels_dir = os.path.join(results_w_category_dir, id, \"labels-pred\")\n",
    "\n",
    "    if not override_visuals and os.path.exists(out_dir):\n",
    "        size = subprocess.check_output(['du','-sh', out_dir]).split()[0].decode('utf-8')\n",
    "        print(f\"Skipping: visuals exist ({size}) for\", x)\n",
    "        continue\n",
    "\n",
    "    command = f\"torchpack dist-run -np 1 python tools/visualize.py \\\n",
    "        {cfg_path} --checkpoint {pth_path} \\\n",
    "        --out-dir {out_dir} \\\n",
    "        --mode pred --split test\"\n",
    "    \n",
    "    if visuals_save_bboxes:\n",
    "        command += \" --save-bboxes --save-labels\"\n",
    "        command += f\" --save-bboxes-dir {bboxes_dir}\"\n",
    "        command += f\" --save-labels-dir {labels_dir}\"\n",
    "    \n",
    "    if x.split(\"/\")[-1][0] in [\"C\", \"c\"]:\n",
    "        command += f\" --bbox-score {visuals_camera_bbox_score}\"\n",
    "\n",
    "    if visuals_max_samples is not None:\n",
    "        command += f\" --max-samples {visuals_max_samples}\"\n",
    "\n",
    "    if visuals_include_combined:\n",
    "        command += \" --include-combined\"\n",
    "\n",
    "    command += \" > /dev/null 2>&1\"\n",
    "\n",
    "    print(\"Creating images for\", x)\n",
    "\n",
    "    os.system(command)\n",
    "\n",
    "    size = subprocess.check_output(['du','-sh', out_dir]).split()[0].decode('utf-8')\n",
    "    print(\"\\tDirectory size: \" + size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create videos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in glob(os.path.join(results_w_category_dir, \"*\")):\n",
    "    out_path = os.path.join(x, \"videos\")\n",
    "    if not override_videos and os.path.exists(out_path):\n",
    "        size = subprocess.check_output(['du','-sh', out_path]).split()[0].decode('utf-8')\n",
    "        print(f\"Skipping: videos exist ({size}) for\", x)\n",
    "        continue\n",
    "\n",
    "    print(\"Creating videos for\", x)\n",
    "    \n",
    "    for source_folder_dir in glob(os.path.join(x, visuals_folder_name, \"*\")):\n",
    "        folder_name = os.path.basename(source_folder_dir)\n",
    "        if not os.path.isdir(source_folder_dir):\n",
    "            continue\n",
    "        target_path = os.path.join(out_path, folder_name + \".mp4\")\n",
    "        command = f\"python tools/visualization/create_video.py -s {source_folder_dir} -t {target_path}\"\n",
    "        os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in chkpts:\n",
    "    target_path = os.path.join(\n",
    "            results_w_category_dir,\n",
    "            os.path.basename(x),\n",
    "            benchmark_filename,\n",
    "        )\n",
    "\n",
    "    cfg_path = os.path.join(x, \"configs.yaml\")\n",
    "    pth_path = os.path.join(x, \"latest.pth\")\n",
    "\n",
    "    if not override_benchmark and os.path.exists(target_path):\n",
    "        print(f\"Skipping: benchmark exists for\", x)\n",
    "        continue\n",
    "\n",
    "    print(\"Benchmarking for\", x)\n",
    "    command = f\"python tools/benchmark.py {cfg_path} {pth_path} --out {target_path}\"\n",
    "    command += \" > /dev/null 2>&1\"\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling\n",
    "\n",
    "---\n",
    "\n",
    "## Result formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERR_NAME_MAPPING = {\n",
    "    \"trans_err\": \"mATE\",\n",
    "    \"scale_err\": \"mASE\",\n",
    "    \"orient_err\": \"mAOE\",\n",
    "}\n",
    "\n",
    "CLASSES = (\n",
    "    \"CAR\",\n",
    "    \"TRAILER\",\n",
    "    \"TRUCK\",\n",
    "    \"VAN\",\n",
    "    \"PEDESTRIAN\",\n",
    "    \"BUS\",\n",
    "    \"MOTORCYCLE\",\n",
    "    \"BICYCLE\",\n",
    "    \"EMERGENCY_VEHICLE\",\n",
    "    # \"OTHER\",\n",
    ")\n",
    "\n",
    "HEADERS = [\n",
    "    \"id\",\n",
    "    \"eval_type\",\n",
    "    \"sensors\",\n",
    "    \"test_fps\",\n",
    "    \"test_mem\",\n",
    "    \"mAP\",\n",
    "    \"mAOE\",\n",
    "    \"mATE\",\n",
    "    \"mASE\",\n",
    "    \n",
    "]\n",
    "\n",
    "for m in [\"mAP\", \"mAOE\", \"mATE\", \"mASE\"]:\n",
    "    for x in CLASSES:\n",
    "        HEADERS.append(f\"{x}_{m}\")\n",
    "\n",
    "HEADERS.append(\"num_gt_bboxes\")\n",
    "\n",
    "for x in CLASSES:\n",
    "    HEADERS.append(f\"num_{x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(glob(os.path.join(results_w_category_dir, \"*\"))):\n",
    "    if not os.path.isdir(x):\n",
    "        continue\n",
    "    print(\"Compiling results for\", x)\n",
    "\n",
    "    id = os.path.basename(x)\n",
    "    path = os.path.join(x, test_results_filename)\n",
    "    with open(path, \"rb\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    out_csv = os.path.join(x, test_results_csv_filename)\n",
    "    with open(out_csv, \"w\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=HEADERS)\n",
    "        writer.writeheader()\n",
    "\n",
    "        sensors = \"\"\n",
    "        if \"lid\" in id.lower():\n",
    "            sensors += \"L\"\n",
    "        elif \"cam\" in id.lower():\n",
    "            sensors += \"C\"\n",
    "        else:\n",
    "            sensors += \"LC\"\n",
    "\n",
    "        # read benchmark data\n",
    "        benchmark_path = os.path.join(x, benchmark_filename)\n",
    "        benchmark_data = None\n",
    "        with open(benchmark_path, \"rb\") as json_file:\n",
    "            benchmark_data = json.load(json_file)\n",
    "\n",
    "        for eval_type, eval_data in data.items():\n",
    "            row = {\n",
    "                \"id\": id,\n",
    "                \"sensors\": sensors,\n",
    "                \"test_fps\": benchmark_data[\"fps\"],\n",
    "                \"test_mem\": benchmark_data[\"memory_allocated\"],\n",
    "                \"eval_type\": eval_type,\n",
    "                \"mAP\": eval_data[\"mean_ap\"],\n",
    "            }\n",
    "\n",
    "            for x in CLASSES:\n",
    "                if x in eval_data[\"class_counts\"]:\n",
    "                    row.update({f\"num_{x}\": eval_data[\"class_counts\"][x]})\n",
    "                else:\n",
    "                    row.update({f\"num_{x}\": \"\"})\n",
    "\n",
    "            for tp_name, tp_val in eval_data[\"tp_errors\"].items():\n",
    "                if tp_name in ERR_NAME_MAPPING:\n",
    "                    row.update({ERR_NAME_MAPPING[tp_name]: tp_val})\n",
    "\n",
    "            class_aps = eval_data[\"mean_dist_aps\"]\n",
    "            class_tps = eval_data[\"label_tp_errors\"]\n",
    "\n",
    "            for class_name in class_aps.keys():\n",
    "                row.update({f\"{class_name}_mAP\": class_aps[class_name]})\n",
    "                for tp_name, tp_val in class_tps[class_name].items():\n",
    "                    if tp_name in ERR_NAME_MAPPING:\n",
    "                        row.update(\n",
    "                            {\n",
    "                                f\"{class_name}_{ERR_NAME_MAPPING[tp_name]}\": class_tps[class_name][\n",
    "                                    tp_name\n",
    "                                ]\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            writer.writerows([row])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile into a summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(test_results_compiled_path), exist_ok=True)\n",
    "with open(test_results_compiled_path, \"w\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=HEADERS)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i, x in enumerate(glob(os.path.join(results_w_category_dir, \"*\"))):\n",
    "        id = os.path.basename(x)\n",
    "        print(\"Compiling results for\", x)\n",
    "        data = None\n",
    "        path = os.path.join(x, test_results_filename)\n",
    "        with open(path, \"rb\") as json_file:\n",
    "            data = json.load(json_file)\n",
    "\n",
    "        sensors = \"\"\n",
    "        if \"lid\" in id.lower():\n",
    "            sensors += \"L\"\n",
    "        elif \"cam\" in id.lower():\n",
    "            sensors += \"C\"\n",
    "        else:\n",
    "            sensors += \"LC\"\n",
    "\n",
    "        # read benchmark data\n",
    "        benchmark_path = os.path.join(x, benchmark_filename)\n",
    "        benchmark_data = None\n",
    "        with open(benchmark_path, \"rb\") as json_file:\n",
    "            benchmark_data = json.load(json_file)\n",
    "\n",
    "        for eval_type, eval_data in data.items():\n",
    "            row = {\n",
    "                \"id\": id,\n",
    "                \"sensors\": sensors,\n",
    "                \"test_fps\": round(benchmark_data[\"fps\"], 2),\n",
    "                \"test_mem\": round(benchmark_data[\"memory_allocated\"], 2),\n",
    "                \"eval_type\": eval_type,\n",
    "                \"mAP\": eval_data[\"mean_ap\"],\n",
    "            }\n",
    "\n",
    "            for x in CLASSES:\n",
    "                if x in eval_data[\"class_counts\"]:\n",
    "                    row.update({f\"num_{x}\": eval_data[\"class_counts\"][x]})\n",
    "                else:\n",
    "                    row.update({f\"num_{x}\": \"\"})\n",
    "\n",
    "            for tp_name, tp_val in eval_data[\"tp_errors\"].items():\n",
    "                if tp_name in ERR_NAME_MAPPING:\n",
    "                    row.update({ERR_NAME_MAPPING[tp_name]: tp_val})\n",
    "\n",
    "            class_aps = eval_data[\"mean_dist_aps\"]\n",
    "            class_tps = eval_data[\"label_tp_errors\"]\n",
    "\n",
    "            for class_name in class_aps.keys():\n",
    "                row.update({f\"{class_name}_mAP\": class_aps[class_name]})\n",
    "                for tp_name, tp_val in class_tps[class_name].items():\n",
    "                    if tp_name in ERR_NAME_MAPPING:\n",
    "                        row.update(\n",
    "                            {\n",
    "                                f\"{class_name}_{ERR_NAME_MAPPING[tp_name]}\": class_tps[class_name][\n",
    "                                    tp_name\n",
    "                                ]\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            row.update({\"num_gt_bboxes\": eval_data[\"total_gt_bboxes\"]})\n",
    "\n",
    "            writer.writerows([row])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
