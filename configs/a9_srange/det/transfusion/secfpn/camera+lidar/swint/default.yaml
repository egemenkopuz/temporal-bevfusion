voxel_size: [0.1, 0.1, 0.2]
point_cloud_range: [-25.0, -75.0, -10.0, 125.0, 75.0, 0.0]

data:
  samples_per_gpu: 6
  workers_per_gpu: 6

model:
  encoders:
    camera:
      backbone:
        type: SwinTransformer
        embed_dims: 96
        depths: [2, 2, 6, 2]
        num_heads: [3, 6, 12, 24]
        window_size: 7
        mlp_ratio: 4
        qkv_bias: true
        qk_scale: null
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.2
        patch_norm: true
        out_indices: [1, 2, 3]
        with_cp: false
        convert_weights: true
        init_cfg:
          type: Pretrained
          checkpoint: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
      neck:
        in_channels: [192, 384, 768]
      vtransform:
        xbound: [-25.0, 125.0, 0.4] # can be changed to 0.4
        ybound: [-75.0, 75.0, 0.4] # can be changed to 0.4
    lidar:
      voxelize:
        point_cloud_range: ${point_cloud_range}
        voxel_size: ${voxel_size}
        max_voxels: [120000, 160000]
      backbone:
        sparse_shape: [1500, 1500, 51]

  heads:
    object:
      train_cfg:
        grid_size: [1500, 1500, 51]
      test_cfg:
        grid_size: [1500, 1500, 51]

lr_config:
  policy: CosineAnnealing
  warmup: linear
  warmup_iters: 500
  warmup_ratio: 0.33333333
  min_lr_ratio: 1.0e-3
